{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rr1d-kHohxGL"
      },
      "outputs": [],
      "source": [
        "!pip install gym[atari,accept-rom-license]==0.25.2\n",
        "import sys, os\n",
        "import gym\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "IGCa_JQeiy1F"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import random\n",
        "from gym.spaces import Box\n",
        "from collections import deque\n",
        "\n",
        "steps = 3 # 3 for multistep learning, 1 for standard\n",
        "\n",
        "class SkipFrame(gym.Wrapper):\n",
        "    def __init__(self, env, skip):\n",
        "        super().__init__(env)\n",
        "        self._skip = skip\n",
        "\n",
        "    def step(self, action):\n",
        "        total_reward = 0.0\n",
        "        done = False\n",
        "        for i in range(self._skip):\n",
        "            obs, reward, done, info = self.env.step(action)\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "        return obs, total_reward, done, info\n",
        "\n",
        "\n",
        "class GrayScaleObservation(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        obs_shape = self.observation_space.shape[:2]\n",
        "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        observation = np.transpose(observation, (2, 0, 1))\n",
        "        observation = torch.tensor(observation.copy(), dtype=torch.float)\n",
        "        transform = torchvision.transforms.Grayscale()\n",
        "        observation = transform(observation)\n",
        "        return observation\n",
        "\n",
        "\n",
        "class ResizeObservation(gym.ObservationWrapper):\n",
        "    def __init__(self, env, shape):\n",
        "        super().__init__(env)\n",
        "        self.shape = (shape, shape) if isinstance(shape, int) else tuple(shape)\n",
        "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
        "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        transforms = torchvision.transforms.Compose([torchvision.transforms.Resize(self.shape),\n",
        "                                                     torchvision.transforms.Normalize(0, 255)])\n",
        "        return transforms(observation).squeeze(0)\n",
        "\n",
        "\n",
        "class ExperienceReplayMemory(object):\n",
        "    def __init__(self, capacity):\n",
        "        self.memory = deque([], maxlen=capacity)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "    def store(self, state, next_state, action, reward, done):\n",
        "        state = state.__array__()\n",
        "        next_state = next_state.__array__()\n",
        "\n",
        "        if steps>1 and len(self.memory) >= steps:\n",
        "          r = sum(self.memory[-i][3] * (gamma ** (steps - i)) for i in range(steps, 0, -1))\n",
        "          tmp = self.memory[-(steps)][:3] + (r,) + self.memory[-(steps)][4:]\n",
        "          self.memory[-(steps)] = tmp\n",
        "\n",
        "        self.memory.append((state, next_state, action, reward, done))\n",
        "\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        # TODO: uniformly sample batches of Tensors for: state, next_state, action, reward, done\n",
        "        batch = random.sample(self.memory, batch_size)\n",
        "        state, next_state, action, reward, done = zip(*batch)\n",
        "\n",
        "        state = np.array(state)\n",
        "        next_state = np.array(next_state)\n",
        "\n",
        "        return torch.tensor(state), torch.tensor(next_state), torch.tensor(action), torch.tensor(reward), torch.tensor(done)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOCiUgfBjJYc",
        "outputId": "853a242a-e02e-45ec-9e71-2393de937f8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of stacked frames:  4\n",
            "Resized observation space dimensionality:  84 84\n",
            "Number of available actions by the agent:  4\n",
            "cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import gym\n",
        "import numpy as np\n",
        "import copy\n",
        "from gym.wrappers import FrameStack\n",
        "\n",
        "\n",
        "env_rendering = False    # Set to False while training your model on Colab\n",
        "testing_mode = False\n",
        "test_model_directory = './your_saved_model.pth.tar'\n",
        "run_as_ddqn = True\n",
        "\n",
        "# Create and preprocess the Atari Breakout environment\n",
        "if env_rendering:\n",
        "    env = gym.make(\"ALE/Breakout-v5\", full_action_space=False, render_mode=\"human\")\n",
        "else:\n",
        "    env = gym.make(\"ALE/Breakout-v5\", full_action_space=False)\n",
        "env = SkipFrame(env, skip=4)\n",
        "env = GrayScaleObservation(env)\n",
        "env = ResizeObservation(env, shape=84)\n",
        "env = FrameStack(env, num_stack=4)\n",
        "image_stack, h, w = env.observation_space.shape\n",
        "num_actions = env.action_space.n\n",
        "print('Number of stacked frames: ', image_stack)\n",
        "print('Resized observation space dimensionality: ', h, w)\n",
        "print('Number of available actions by the agent: ', num_actions)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "seed = 61\n",
        "env.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "if torch.backends.cudnn.enabled:\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "# Hyperparameters (to be modified\n",
        "batch_size = 20\n",
        "alpha = 0.00025\n",
        "gamma = 0.95\n",
        "eps, eps_decay, min_eps = 1.0, 0.999, 0.05\n",
        "buffer = ExperienceReplayMemory(5000)\n",
        "burn_in_phase = 1000\n",
        "sync_target = 2000\n",
        "max_train_frames = 1500\n",
        "max_train_episodes = 4000\n",
        "max_test_episodes = 20\n",
        "curr_step = 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "NNoUgNjujqRX"
      },
      "outputs": [],
      "source": [
        "def convert(x):\n",
        "    return torch.tensor(x.__array__()).float()\n",
        "\n",
        "\n",
        "class DeepQNet(torch.nn.Module):\n",
        "    def __init__(self, h, w, image_stack, num_actions):\n",
        "        super(DeepQNet, self).__init__()\n",
        "        # TODO: create a convolutional neural network\n",
        "\n",
        "        self.conv1 = torch.nn.Conv2d(in_channels=image_stack, out_channels=32, kernel_size=8, stride=4)\n",
        "\n",
        "        self.act1 = torch.nn.ReLU()\n",
        "\n",
        "        self.conv2 = torch.nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2)\n",
        "\n",
        "        self.act2 = torch.nn.ReLU()\n",
        "\n",
        "        self.conv3 = torch.nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1)\n",
        "\n",
        "        self.act3 = torch.nn.ReLU()\n",
        "\n",
        "\n",
        "        self.flat = torch.nn.Flatten()\n",
        "\n",
        "        self.fc1 = torch.nn.Linear(64 * 7 * 7, 256)\n",
        "\n",
        "        self.act4 = torch.nn.ReLU()\n",
        "\n",
        "        self.fc2 = torch.nn.Linear(256, num_actions)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO: forward pass from the neural network\n",
        "        x = self.conv1(x)\n",
        "        x = self.act1(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.act2(x)\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x = self.act3(x)\n",
        "\n",
        "        x = self.flat(x)\n",
        "\n",
        "        x = self.fc1(x)\n",
        "        x = self.act4(x)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# TODO: create an online and target DQN (Hint: Use copy.deepcopy() and requires_grad utilities!)\n",
        "# ...\n",
        "online_dqn = DeepQNet(h, w, image_stack, num_actions)\n",
        "target_dqn = copy.deepcopy(online_dqn)\n",
        "for p in target_dqn.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "online_dqn.to(device)\n",
        "target_dqn.to(device)\n",
        "\n",
        "\n",
        "# TODO: create the appropriate MSE criterion and Adam optimizer\n",
        "# ...\n",
        "optimizer = torch.optim.Adam(online_dqn.parameters(), lr=alpha)\n",
        "\n",
        "# Set this variable to False to run HuberLoss criterion in task 1.c)\n",
        "mse_vs_hubber = True\n",
        "\n",
        "if mse_vs_hubber:\n",
        "  criterion = torch.nn.MSELoss()\n",
        "else:\n",
        "  criterion = torch.nn.HuberLoss()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "rkkjUQI0YOwf"
      },
      "outputs": [],
      "source": [
        "def policy(state, is_training):\n",
        "    global eps\n",
        "    state = convert(state).unsqueeze(0).to(device)\n",
        "\n",
        "    # TODO: Implement an epsilon-greedy policy\n",
        "    # ...\n",
        "    if is_training and np.random.rand() < eps:\n",
        "        action = torch.randint(0, num_actions, (1,), dtype=torch.long)\n",
        "    elif is_training:\n",
        "        q_vals = online_dqn(state)\n",
        "        action = torch.argmax(q_vals).item()\n",
        "    else:\n",
        "        with torch.no_grad():\n",
        "            q_vals = online_dqn(state)\n",
        "            action = torch.argmax(q_vals).item()\n",
        "    return torch.tensor(action)\n",
        "\n",
        "\n",
        "def compute_loss(state, action, reward, next_state, done):\n",
        "    state = convert(state).to(device)\n",
        "    next_state = convert(next_state).to(device)\n",
        "    action = action.to(device)\n",
        "    reward = torch.tensor(reward).to(device)\n",
        "    done = torch.tensor(done).to(device)\n",
        "\n",
        "    # TODO: Compute the DQN (or DDQN) loss based on the criterion\n",
        "\n",
        "    if testing_mode:\n",
        "      state = state.unsqueeze(0)\n",
        "      next_state = next_state.unsqueeze(0)\n",
        "\n",
        "    action = action.view(-1,1)\n",
        "    reward = reward.view(-1,1)\n",
        "    done = done.view(-1,1)\n",
        "    done = done.int()\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "      if run_as_ddqn:\n",
        "        next_q_vals_1 = online_dqn(next_state)\n",
        "        max_next_q_vals_1 = torch.argmax(next_q_vals_1,dim=1).view(-1,1)\n",
        "        next_q_vals_2 = target_dqn(next_state)\n",
        "        max_next_q_vals = next_q_vals_2.gather(1,max_next_q_vals_1)\n",
        "      else:\n",
        "        next_q_vals = target_dqn(next_state)\n",
        "        max_next_q_vals = torch.max(next_q_vals, dim=1)[0]\n",
        "        max_next_q_vals = max_next_q_vals.view(-1,1)\n",
        "        max_next_q_vals = max_next_q_vals.detach()\n",
        "\n",
        "    target_q = reward + (gamma ** (steps)) * max_next_q_vals * (1-done)\n",
        "\n",
        "    prediction_current_q = online_dqn(state).gather(1, action)\n",
        "\n",
        "    loss = criterion(prediction_current_q, target_q)\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "def run_episode(curr_step, buffer, is_training):\n",
        "    global eps\n",
        "    global target_dqn\n",
        "    episode_reward, episode_loss = 0, 0.\n",
        "    state = env.reset()\n",
        "\n",
        "\n",
        "    for t in range(max_train_frames):\n",
        "        action = policy(state, is_training)\n",
        "        curr_step += 1\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        episode_reward += reward\n",
        "\n",
        "        if is_training:\n",
        "            buffer.store(state, next_state, action, reward, done)\n",
        "\n",
        "            if curr_step > burn_in_phase:\n",
        "                state_batch, next_state_batch, action_batch, reward_batch, done_batch = buffer.sample(batch_size)\n",
        "\n",
        "                if curr_step % sync_target == 0:\n",
        "                    # TODO: Periodically update your target_dqn at each sync_target frames\n",
        "                    # ...\n",
        "                    target_dqn.load_state_dict(online_dqn.state_dict())\n",
        "\n",
        "                loss = compute_loss(state_batch, action_batch, reward_batch, next_state_batch, done_batch)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                episode_loss += loss.item()\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                episode_loss += compute_loss(state, action, reward, next_state, done).item()\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    return dict(reward=episode_reward, loss=episode_loss / t), curr_step\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "PEGSGYsQj8Wh"
      },
      "outputs": [],
      "source": [
        "def update_metrics(metrics, episode):\n",
        "    for k, v in episode.items():\n",
        "        metrics[k].append(v)\n",
        "\n",
        "\n",
        "def print_metrics(it, metrics, is_training, window=100):\n",
        "    reward_mean = np.mean(metrics['reward'][-window:])\n",
        "    loss_mean = np.mean(metrics['loss'][-window:])\n",
        "    mode = \"train\" if is_training else \"test\"\n",
        "    print(f\"Episode {it:4d} | {mode:5s} | reward {reward_mean:5.5f} | loss {loss_mean:5.5f}\")\n",
        "\n",
        "\n",
        "def save_checkpoint(curr_step, eps, train_metrics):\n",
        "    save_dict = {'curr_step': curr_step,\n",
        "                 'train_metrics': train_metrics,\n",
        "                 'eps': eps,\n",
        "                 'online_dqn': online_dqn.state_dict(),\n",
        "                 'target_dqn': target_dqn.state_dict()}\n",
        "    torch.save(save_dict, './your_saved_model.pth.tar')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Uy7C4qfWkzXb"
      },
      "outputs": [],
      "source": [
        "# TODO: Plot your train_metrics and test_metrics\n",
        "# ...\n",
        "def plot_metrics(name, metrics):\n",
        "\n",
        "  fig, ax_list = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "  fig.suptitle('Atari game results - ' + name, fontsize=18)\n",
        "\n",
        "  ax_list[0].plot(metrics['reward'], lw=2, color='red')\n",
        "  ax_list[0].set_ylabel('Accumulated reward')\n",
        "  ax_list[0].set_xlabel('Episode')\n",
        "\n",
        "  if name.startswith('Testing'):\n",
        "    ax_list[0].axhline(np.mean(metrics['reward']), color='black', linestyle='dashed',lw=3)\n",
        "\n",
        "  ax_list[1].plot(metrics['loss'], lw=2, color='red')\n",
        "  ax_list[1].set_xlabel('Episode')\n",
        "  ax_list[1].set_ylabel('Loss')\n",
        "\n",
        "  plt.tight_layout(pad=2.0)\n",
        "\n",
        "  plt.savefig(f'Atarigame_{name}.pdf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TdhWcoW-kyuF"
      },
      "outputs": [],
      "source": [
        "tmp = \"ddqn\" if run_as_ddqn else \"dqn\"\n",
        "var = tmp + str(steps)\n",
        "print(var)\n",
        "if testing_mode:\n",
        "    # TODO: Load your saved online_dqn model for evaluation\n",
        "    # ...\n",
        "    checkpoint = torch.load(test_model_directory)\n",
        "    online_dqn.load_state_dict(checkpoint['online_dqn'])\n",
        "\n",
        "    test_metrics = dict(reward=[], loss=[])\n",
        "    for it in range(max_test_episodes):\n",
        "        episode_metrics, curr_step = run_episode(curr_step, buffer, is_training=False)\n",
        "        update_metrics(test_metrics, episode_metrics)\n",
        "        print_metrics(it + 1, test_metrics, is_training=False)\n",
        "    plot_metrics(\"Testing \"+var, test_metrics)\n",
        "else:\n",
        "    train_metrics = dict(reward=[], loss=[])\n",
        "    for it in range(max_train_episodes):\n",
        "        episode_metrics, curr_step = run_episode(curr_step, buffer, is_training=True)\n",
        "        update_metrics(train_metrics, episode_metrics)\n",
        "        if curr_step > burn_in_phase and eps > min_eps:\n",
        "            eps *= eps_decay\n",
        "        if it % 50 == 0:\n",
        "            print_metrics(it, train_metrics, is_training=True)\n",
        "            save_checkpoint(curr_step, eps, train_metrics)\n",
        "\n",
        "    plot_metrics(\"Training \"+var, train_metrics)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}